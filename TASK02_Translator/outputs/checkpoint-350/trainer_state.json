{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 350,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 274.4707946777344,
      "learning_rate": 4.957142857142857e-05,
      "loss": 11.6699,
      "step": 10
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 272.8720397949219,
      "learning_rate": 4.8142857142857147e-05,
      "loss": 8.4078,
      "step": 20
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 221.6263427734375,
      "learning_rate": 4.671428571428571e-05,
      "loss": 4.2924,
      "step": 30
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 254.5238037109375,
      "learning_rate": 4.542857142857143e-05,
      "loss": 3.0956,
      "step": 40
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 474.3311767578125,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.2509,
      "step": 50
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 220.55490112304688,
      "learning_rate": 4.257142857142857e-05,
      "loss": 1.788,
      "step": 60
    },
    {
      "epoch": 1.0,
      "grad_norm": 146.58290100097656,
      "learning_rate": 4.1142857142857146e-05,
      "loss": 1.5915,
      "step": 70
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 115.77981567382812,
      "learning_rate": 3.971428571428571e-05,
      "loss": 1.5228,
      "step": 80
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 213.65292358398438,
      "learning_rate": 3.8285714285714286e-05,
      "loss": 1.4996,
      "step": 90
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 153.4307861328125,
      "learning_rate": 3.685714285714286e-05,
      "loss": 1.57,
      "step": 100
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 95.1821517944336,
      "learning_rate": 3.5428571428571426e-05,
      "loss": 1.3824,
      "step": 110
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 109.31922912597656,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.5284,
      "step": 120
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 77.75419616699219,
      "learning_rate": 3.257142857142857e-05,
      "loss": 1.4169,
      "step": 130
    },
    {
      "epoch": 2.0,
      "grad_norm": 70.86461639404297,
      "learning_rate": 3.114285714285715e-05,
      "loss": 1.3636,
      "step": 140
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 157.82925415039062,
      "learning_rate": 2.9714285714285717e-05,
      "loss": 1.3608,
      "step": 150
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 137.7266387939453,
      "learning_rate": 2.8285714285714287e-05,
      "loss": 1.4234,
      "step": 160
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 51.236228942871094,
      "learning_rate": 2.6857142857142857e-05,
      "loss": 1.3496,
      "step": 170
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 113.24862670898438,
      "learning_rate": 2.542857142857143e-05,
      "loss": 1.377,
      "step": 180
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 87.8082504272461,
      "learning_rate": 2.4e-05,
      "loss": 1.36,
      "step": 190
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 203.5749053955078,
      "learning_rate": 2.257142857142857e-05,
      "loss": 1.3809,
      "step": 200
    },
    {
      "epoch": 3.0,
      "grad_norm": 45.99701690673828,
      "learning_rate": 2.1142857142857144e-05,
      "loss": 1.1324,
      "step": 210
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 74.88018035888672,
      "learning_rate": 1.9714285714285714e-05,
      "loss": 1.3207,
      "step": 220
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 73.88630676269531,
      "learning_rate": 1.8285714285714288e-05,
      "loss": 1.3055,
      "step": 230
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 66.68683624267578,
      "learning_rate": 1.6857142857142858e-05,
      "loss": 1.2597,
      "step": 240
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 67.78873443603516,
      "learning_rate": 1.5428571428571428e-05,
      "loss": 1.1807,
      "step": 250
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 79.46631622314453,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.1694,
      "step": 260
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 62.3858642578125,
      "learning_rate": 1.2571428571428573e-05,
      "loss": 1.3,
      "step": 270
    },
    {
      "epoch": 4.0,
      "grad_norm": 136.3395233154297,
      "learning_rate": 1.1142857142857143e-05,
      "loss": 1.2893,
      "step": 280
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 59.672061920166016,
      "learning_rate": 9.714285714285715e-06,
      "loss": 1.2181,
      "step": 290
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 71.31068420410156,
      "learning_rate": 8.285714285714285e-06,
      "loss": 1.1289,
      "step": 300
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 61.33387756347656,
      "learning_rate": 6.857142857142858e-06,
      "loss": 1.1575,
      "step": 310
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 68.82283782958984,
      "learning_rate": 5.428571428571429e-06,
      "loss": 1.2575,
      "step": 320
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 90.51776123046875,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.2723,
      "step": 330
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 58.90104293823242,
      "learning_rate": 2.5714285714285716e-06,
      "loss": 1.2,
      "step": 340
    },
    {
      "epoch": 5.0,
      "grad_norm": 74.41018676757812,
      "learning_rate": 1.142857142857143e-06,
      "loss": 1.1944,
      "step": 350
    }
  ],
  "logging_steps": 10,
  "max_steps": 350,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.438222569668608e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
